It has been on my list to make a public version of testing and debugging
code including unit testing so I found this topic particularly intriguing.

Since something related is starting here and I have reviewed most of the 
listed utilities before I thought I would just list some functionality
unique to what I use as food for thought ...

Specifically focusing on tests comprising a unit test (versus parameter
checking and other run-time testing functionality) a few differences between
the referenced unit testers and what I like in my version are:

    There is a number of global parameters that are set external to the 
    main "assert" routine (I call mine UNIT_CHECK):

    * A "level" value which is just an integer that all the tests can
      access that the user is free to use to determine what level of
      testing to perform and/or how verbose to be.

    * As mentioned, an option to make a failure stop the test program
      or to optionally continue (some of the references have this,
      but as an option on "assert", not as a global mode).

    * An option to skip producing messages when successful to make the output
      more succinct (some of the references have this, but as an option on
      "assert", not as a global mode).

In tests that I call that I want part of the run-time production routines
I want options like this to be parameters on the call to the "assert"
test routine, but in unit testing I want these to be run-time selectable
global options for the entire set of tests.

Other things I found different

* A name (almost always the procedure name being tested) is required as
  the first parameter in my "assert" routine.

* One I am pretty sure is totally unique is an optional command name
  to call when starting and stopping a test set.
  This lets you arbitrarily enter test results into an SQLite
  database file, a CSV log, send mail messages, or whatever arbitrary
  action you want, versus just printing a simple log or exiting with
  a non-zero exit status -- you provide the command.

* the "message field" is actually the "message fields" on mine, which is
  up to nine polymorphic variables so you can easily include numeric
  and logical values in the message, not just strings. Requires Modern
  Fortran, of course -- but a major convenience.

## FOR MORE INFORMATION ON M_DEBUG

Some unpolished parts of what I had in mind for what I was going to
release have already been extracted and made available in the [General
Purpose Fortran](https://github.com/urbanjost?tab=repositories)
repository, primarily in the M_debug module, as I needed some basic
testing functionallity just to start GPF. __So I agree based on experience
that the functionality of unit testing is needed pretty early on in
stdlib__.

I usually mix the unit tests in M_debug  with generic routines similar
to the dp_accdig() routine in M_math (to compare REAL values with a
tolerance) and the ufpp(1) preprocessor $SYSTEM directive and the
numdiff(1) program... so M_debug is intentionally simple and not
complete by itself, but is more a framework than a full utility. I
think that is a good model -- keep the unit test routines simple and
use generic routines for comparing reals. These routines can be called
in the expression field of "assert" instead of being provided as a set
of routines. Powerful testing intrinsics like ANY() and ALL() already
exist and I do not think need re-invented.

Note that if anyone builds the GPF repository it tries to run at least a
few thousand tests at the end so there might not be a lot of documenation
but there are a lot of examples, at least if you use gfortran on a
*nix system (which is the default expected environment).

So lots of other things are possible, but normally you call very simple routines
like ...

   unit_check_start(...
   unit_check(name,expression,messages(s)...)
   unit_check(name,expression,messages(s)...)
   unit_check(name,expression,messages(s)...)
   unit_check_done(...

Generally for quick confidence tests to make sure things did not accidentally 
change I find (in practice) that I just let it procedure basic messages,
resulting in a simple log like:

    STARTED test_suite_m_sort
    unit_check:       sort_shell           SUCCESS : sort string array, ascending
    unit_check:       sort_shell           SUCCESS : sort string array, descending
    unit_check:       sort_shell           SUCCESS : sort integer, ascending array
    unit_check:       sort_shell           SUCCESS : sort integer, descending array
    unit_check:       sort_shell           SUCCESS : sort real, ascending
    unit_check:       sort_shell           SUCCESS : sort real, descending
    unit_check:       sort_shell           SUCCESS : sort doubleprecision, ascending
    unit_check:       sort_shell           SUCCESS : sort doubleprecision, descending
    unit_check:       sort_shell           SUCCESS : sort complex by real component, ascending
    unit_check:       sort_shell           SUCCESS : sort complex by real component, descending
    unit_check:       sort_shell           SUCCESS : sort complex by imaginary component, ascending
    unit_check:       sort_shell           SUCCESS : sort complex by imaginary component, descending
    unit_check:       sort_shell           SUCCESS : sort complex array by magnitude, ascending
    unit_check:       sort_shell           SUCCESS : sort complex array by magnitude, descending
    unit_check:       sort_shell           SUCCESS : sort double complex by real component, ascending
    unit_check:       sort_shell           SUCCESS : sort double complex by real component, descending
    unit_check:       sort_shell           SUCCESS : sort double complex by imaginary component, ascending
    unit_check:       sort_shell           SUCCESS : sort double complex by imaginary component, descending
    unit_check:       sort_shell           SUCCESS : sort double complex by magnitude, ascending
    unit_check:       sort_shell           SUCCESS : sort double complex by magnitude, descending
    unit_check_done:  sort_shell           PASSED  : GOOD:20  BAD:0
                          :
                          :
    unit_check:       cosd                 SUCCESS : cosd 1.00000000 1.00000000 value= 0.00000000 accuracy= 1.35631564E-19 asked for 5 digits
    unit_check:       cosd                 SUCCESS : cosd 0.866025388 0.866025388 value= 30.0000000 accuracy= 1.35631564E-19 asked for 5 digits
    unit_check:       cosd                 SUCCESS : cosd 0.707106769 0.707106769 value= 45.0000000 accuracy= 1.35631564E-19 asked for 5 digits
    unit_check:       cosd                 SUCCESS : cosd 0.500000000 0.499999970 value= 60.0000000 accuracy= 1.35631564E-19 asked for 5 digits
    unit_check:       cosd                 SUCCESS : cosd -3.48994955E-02 -3.48994620E-02 value= 92.0000000 accuracy= 6.01741600 asked for 5 digits

I have mine set up so a "test_suite" routine is searched for and
automatically called at the end of the build that uses a bash shell that
calls nm to find the test routine names and then builds a test program
to call the tests, but that is a whole other discussion, I think.
